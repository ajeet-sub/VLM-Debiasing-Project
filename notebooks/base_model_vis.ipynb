{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/hice1/awagh31/VLM-Debiasing-Project-1/scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/956136/ipykernel_1506022/2162656668.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import model as m\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import loaders\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_csv = pd.read_csv(\"/home/hice1/awagh31/scratch/scripts/labels_visual_files_dense.csv\")\n",
    "#features_csv['modalities'] = 'video'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>split</th>\n",
       "      <th>PTSD_label</th>\n",
       "      <th>age</th>\n",
       "      <th>PTSD_severity</th>\n",
       "      <th>file_path</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>22.0</td>\n",
       "      <td>/home/hice1/awagh31/scratch/original/data_unta...</td>\n",
       "      <td>/home/hice1/awagh31/scratch/final_embeddings_v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>23.0</td>\n",
       "      <td>/home/hice1/awagh31/scratch/original/data_unta...</td>\n",
       "      <td>/home/hice1/awagh31/scratch/final_embeddings_v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>19.0</td>\n",
       "      <td>/home/hice1/awagh31/scratch/original/data_unta...</td>\n",
       "      <td>/home/hice1/awagh31/scratch/final_embeddings_v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>67.0</td>\n",
       "      <td>/home/hice1/awagh31/scratch/original/data_unta...</td>\n",
       "      <td>/home/hice1/awagh31/scratch/final_embeddings_v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>dev</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>39.0</td>\n",
       "      <td>/home/hice1/awagh31/scratch/original/data_unta...</td>\n",
       "      <td>/home/hice1/awagh31/scratch/final_embeddings_v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  split  PTSD_label  age  PTSD_severity  \\\n",
       "0  female  train           0   45           22.0   \n",
       "1    male   test           0   69           23.0   \n",
       "2    male  train           0   25           19.0   \n",
       "3  female  train           1   58           67.0   \n",
       "4    male    dev           0   33           39.0   \n",
       "\n",
       "                                           file_path  \\\n",
       "0  /home/hice1/awagh31/scratch/original/data_unta...   \n",
       "1  /home/hice1/awagh31/scratch/original/data_unta...   \n",
       "2  /home/hice1/awagh31/scratch/original/data_unta...   \n",
       "3  /home/hice1/awagh31/scratch/original/data_unta...   \n",
       "4  /home/hice1/awagh31/scratch/original/data_unta...   \n",
       "\n",
       "                                               video  \n",
       "0  /home/hice1/awagh31/scratch/final_embeddings_v...  \n",
       "1  /home/hice1/awagh31/scratch/final_embeddings_v...  \n",
       "2  /home/hice1/awagh31/scratch/final_embeddings_v...  \n",
       "3  /home/hice1/awagh31/scratch/final_embeddings_v...  \n",
       "4  /home/hice1/awagh31/scratch/final_embeddings_v...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality 1 shape: torch.Size([16, 700, 1])\n",
      "Labels shape: tensor([22., 19., 67., 17., 25., 25., 50., 34., 19., 26., 28., 44., 61., 21.,\n",
      "        67., 36.])\n"
     ]
    }
   ],
   "source": [
    "train_loader = loaders.MultiModalityDataset(features_csv[features_csv[\"split\"]==\"train\"], \n",
    "                                            modalities = {\"video\"}, label = \"PTSD_severity\")\n",
    "dataloader = DataLoader(train_loader, batch_size=16, collate_fn=loaders.collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    modalities, labels = batch\n",
    "    print(f\"Modality 1 shape: {modalities[0].shape}\")  # Expected shape: (batch_size, feature, 1)\n",
    "    #print(f\"Modality 2 shape: {modalities[1].shape}\")\n",
    "    #print(f\"Modality 3 shape: {modalities[2].shape}\")\n",
    "    print(f\"Labels shape: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input dimensions for each modality\n",
    "input_dims = [768]  # These are the feature dimensions for each modality\n",
    "\n",
    "# Initialize MultiModalPerceiver model\n",
    "model = m.MultiModalPerceiver(\n",
    "    input_dims=input_dims,\n",
    "    input_channels=1,\n",
    "    input_axis=1,\n",
    "    projection_dim=256,\n",
    "    num_latents=16,\n",
    "    latent_dim=128,\n",
    "    depth=8,\n",
    "    cross_heads=8,\n",
    "    latent_heads=8,\n",
    "    cross_dim_head=32,\n",
    "    latent_dim_head=32,\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.0,\n",
    "    output_dim=1,\n",
    "    weight_tie_layers=True,\n",
    "    fourier_encode_data=False,\n",
    "    max_freq=10,\n",
    "    num_freq_bands=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 38.4536\n",
      "Epoch [1/50], Loss: 34.4295\n",
      "Epoch [1/50], Loss: 33.8099\n",
      "Epoch [1/50], Loss: 31.3196\n",
      "Epoch [1/50], Loss: 37.0714\n",
      "Epoch [1/50], Loss: 31.9935\n",
      "Epoch [1/50], Loss: 26.4434\n",
      "Epoch [1/50], Loss: 40.2472\n",
      "Epoch [1/50], Loss: 38.6135\n",
      "Epoch [1/50], Loss: 31.0167\n",
      "Epoch [1/50], Loss: 15.4004\n",
      "Epoch [2/50], Loss: 32.2602\n",
      "Epoch [2/50], Loss: 30.8392\n",
      "Epoch [2/50], Loss: 31.4963\n",
      "Epoch [2/50], Loss: 29.6898\n",
      "Epoch [2/50], Loss: 35.7483\n",
      "Epoch [2/50], Loss: 30.7799\n",
      "Epoch [2/50], Loss: 25.2887\n",
      "Epoch [2/50], Loss: 39.0367\n",
      "Epoch [2/50], Loss: 37.3092\n",
      "Epoch [2/50], Loss: 29.7565\n",
      "Epoch [2/50], Loss: 14.0532\n",
      "Epoch [3/50], Loss: 31.0144\n",
      "Epoch [3/50], Loss: 29.5150\n",
      "Epoch [3/50], Loss: 30.2107\n",
      "Epoch [3/50], Loss: 28.4880\n",
      "Epoch [3/50], Loss: 34.5426\n",
      "Epoch [3/50], Loss: 29.5496\n",
      "Epoch [3/50], Loss: 24.0889\n",
      "Epoch [3/50], Loss: 37.7628\n",
      "Epoch [3/50], Loss: 35.9424\n",
      "Epoch [3/50], Loss: 28.4573\n",
      "Epoch [3/50], Loss: 12.6757\n",
      "Epoch [4/50], Loss: 29.7267\n",
      "Epoch [4/50], Loss: 28.1241\n",
      "Epoch [4/50], Loss: 28.8503\n",
      "Epoch [4/50], Loss: 27.2177\n",
      "Epoch [4/50], Loss: 33.2572\n",
      "Epoch [4/50], Loss: 28.2380\n",
      "Epoch [4/50], Loss: 22.8224\n",
      "Epoch [4/50], Loss: 36.3988\n",
      "Epoch [4/50], Loss: 34.4753\n",
      "Epoch [4/50], Loss: 27.0798\n",
      "Epoch [4/50], Loss: 11.2372\n",
      "Epoch [5/50], Loss: 28.3713\n",
      "Epoch [5/50], Loss: 26.6519\n",
      "Epoch [5/50], Loss: 27.4167\n",
      "Epoch [5/50], Loss: 25.8944\n",
      "Epoch [5/50], Loss: 31.9102\n",
      "Epoch [5/50], Loss: 26.8668\n",
      "Epoch [5/50], Loss: 21.5153\n",
      "Epoch [5/50], Loss: 34.9593\n",
      "Epoch [5/50], Loss: 32.9174\n",
      "Epoch [5/50], Loss: 25.6356\n",
      "Epoch [5/50], Loss: 9.7697\n",
      "Epoch [6/50], Loss: 26.9559\n",
      "Epoch [6/50], Loss: 25.1027\n",
      "Epoch [6/50], Loss: 25.9160\n",
      "Epoch [6/50], Loss: 24.5308\n",
      "Epoch [6/50], Loss: 30.5119\n",
      "Epoch [6/50], Loss: 25.4511\n",
      "Epoch [6/50], Loss: 20.1933\n",
      "Epoch [6/50], Loss: 33.4585\n",
      "Epoch [6/50], Loss: 31.2830\n",
      "Epoch [6/50], Loss: 24.1472\n",
      "Epoch [6/50], Loss: 8.3435\n",
      "Epoch [7/50], Loss: 25.5042\n",
      "Epoch [7/50], Loss: 23.4979\n",
      "Epoch [7/50], Loss: 24.3710\n",
      "Epoch [7/50], Loss: 23.1553\n",
      "Epoch [7/50], Loss: 29.0852\n",
      "Epoch [7/50], Loss: 24.0181\n",
      "Epoch [7/50], Loss: 18.8947\n",
      "Epoch [7/50], Loss: 31.9180\n",
      "Epoch [7/50], Loss: 29.5927\n",
      "Epoch [7/50], Loss: 22.6450\n",
      "Epoch [7/50], Loss: 7.0828\n",
      "Epoch [8/50], Loss: 24.0493\n",
      "Epoch [8/50], Loss: 21.8705\n",
      "Epoch [8/50], Loss: 22.8170\n",
      "Epoch [8/50], Loss: 21.8092\n",
      "Epoch [8/50], Loss: 27.6647\n",
      "Epoch [8/50], Loss: 22.6087\n",
      "Epoch [8/50], Loss: 17.6736\n",
      "Epoch [8/50], Loss: 30.3719\n",
      "Epoch [8/50], Loss: 27.8813\n",
      "Epoch [8/50], Loss: 21.1742\n",
      "Epoch [8/50], Loss: 6.1929\n",
      "Epoch [9/50], Loss: 22.6369\n",
      "Epoch [9/50], Loss: 20.2686\n",
      "Epoch [9/50], Loss: 21.3035\n",
      "Epoch [9/50], Loss: 20.5462\n",
      "Epoch [9/50], Loss: 26.2971\n",
      "Epoch [9/50], Loss: 21.2768\n",
      "Epoch [9/50], Loss: 16.5965\n",
      "Epoch [9/50], Loss: 28.8683\n",
      "Epoch [9/50], Loss: 26.2003\n",
      "Epoch [9/50], Loss: 19.7948\n",
      "Epoch [9/50], Loss: 5.9100\n",
      "Epoch [10/50], Loss: 21.3255\n",
      "Epoch [10/50], Loss: 18.7562\n",
      "Epoch [10/50], Loss: 19.8931\n",
      "Epoch [10/50], Loss: 19.4269\n",
      "Epoch [10/50], Loss: 25.0375\n",
      "Epoch [10/50], Loss: 20.0840\n",
      "Epoch [10/50], Loss: 15.7293\n",
      "Epoch [10/50], Loss: 27.4654\n",
      "Epoch [10/50], Loss: 24.6143\n",
      "Epoch [10/50], Loss: 18.5730\n",
      "Epoch [10/50], Loss: 6.3025\n",
      "Epoch [11/50], Loss: 20.1747\n",
      "Epoch [11/50], Loss: 17.4015\n",
      "Epoch [11/50], Loss: 18.6479\n",
      "Epoch [11/50], Loss: 18.5018\n",
      "Epoch [11/50], Loss: 23.9342\n",
      "Epoch [11/50], Loss: 19.0798\n",
      "Epoch [11/50], Loss: 15.1116\n",
      "Epoch [11/50], Loss: 26.2134\n",
      "Epoch [11/50], Loss: 23.1811\n",
      "Epoch [11/50], Loss: 17.5576\n",
      "Epoch [11/50], Loss: 7.1720\n",
      "Epoch [12/50], Loss: 19.2242\n",
      "Epoch [12/50], Loss: 16.2546\n",
      "Epoch [12/50], Loss: 17.6087\n",
      "Epoch [12/50], Loss: 17.7932\n",
      "Epoch [12/50], Loss: 23.0138\n",
      "Epoch [12/50], Loss: 18.2864\n",
      "Epoch [12/50], Loss: 14.7418\n",
      "Epoch [12/50], Loss: 25.1400\n",
      "Epoch [12/50], Loss: 21.9352\n",
      "Epoch [12/50], Loss: 16.7649\n",
      "Epoch [12/50], Loss: 8.2512\n",
      "Epoch [13/50], Loss: 18.4833\n",
      "Epoch [13/50], Loss: 15.3358\n",
      "Epoch [13/50], Loss: 16.7876\n",
      "Epoch [13/50], Loss: 17.2931\n",
      "Epoch [13/50], Loss: 22.2798\n",
      "Epoch [13/50], Loss: 17.6986\n",
      "Epoch [13/50], Loss: 14.5836\n",
      "Epoch [13/50], Loss: 24.2525\n",
      "Epoch [13/50], Loss: 20.8900\n",
      "Epoch [13/50], Loss: 16.1838\n",
      "Epoch [13/50], Loss: 9.3510\n",
      "Epoch [14/50], Loss: 17.9377\n",
      "Epoch [14/50], Loss: 14.6392\n",
      "Epoch [14/50], Loss: 16.1728\n",
      "Epoch [14/50], Loss: 16.9720\n",
      "Epoch [14/50], Loss: 21.7191\n",
      "Epoch [14/50], Loss: 17.2912\n",
      "Epoch [14/50], Loss: 14.5806\n",
      "Epoch [14/50], Loss: 23.5447\n",
      "Epoch [14/50], Loss: 20.0440\n",
      "Epoch [14/50], Loss: 15.7849\n",
      "Epoch [14/50], Loss: 10.3615\n",
      "Epoch [15/50], Loss: 17.5614\n",
      "Epoch [15/50], Loss: 14.1457\n",
      "Epoch [15/50], Loss: 15.7391\n",
      "Epoch [15/50], Loss: 16.7887\n",
      "Epoch [15/50], Loss: 21.3096\n",
      "Epoch [15/50], Loss: 17.0295\n",
      "Epoch [15/50], Loss: 14.6715\n",
      "Epoch [15/50], Loss: 23.0026\n",
      "Epoch [15/50], Loss: 19.3853\n",
      "Epoch [15/50], Loss: 15.5288\n",
      "Epoch [15/50], Loss: 11.2278\n",
      "Epoch [16/50], Loss: 17.3159\n",
      "Epoch [16/50], Loss: 13.8192\n",
      "Epoch [16/50], Loss: 15.4568\n",
      "Epoch [16/50], Loss: 16.7010\n",
      "Epoch [16/50], Loss: 21.0145\n",
      "Epoch [16/50], Loss: 16.8685\n",
      "Epoch [16/50], Loss: 14.8037\n",
      "Epoch [16/50], Loss: 22.6146\n",
      "Epoch [16/50], Loss: 18.9032\n",
      "Epoch [16/50], Loss: 15.3744\n",
      "Epoch [16/50], Loss: 11.9268\n",
      "Epoch [17/50], Loss: 17.1721\n",
      "Epoch [17/50], Loss: 13.6391\n",
      "Epoch [17/50], Loss: 15.3046\n",
      "Epoch [17/50], Loss: 16.6720\n",
      "Epoch [17/50], Loss: 20.8318\n",
      "Epoch [17/50], Loss: 16.7798\n",
      "Epoch [17/50], Loss: 14.9457\n",
      "Epoch [17/50], Loss: 22.3137\n",
      "Epoch [17/50], Loss: 18.5402\n",
      "Epoch [17/50], Loss: 15.2862\n",
      "Epoch [17/50], Loss: 12.4830\n",
      "Epoch [18/50], Loss: 17.0675\n",
      "Epoch [18/50], Loss: 13.4787\n",
      "Epoch [18/50], Loss: 15.1759\n",
      "Epoch [18/50], Loss: 16.6658\n",
      "Epoch [18/50], Loss: 20.7075\n",
      "Epoch [18/50], Loss: 16.7339\n",
      "Epoch [18/50], Loss: 15.0701\n",
      "Epoch [18/50], Loss: 22.0847\n",
      "Epoch [18/50], Loss: 18.2505\n",
      "Epoch [18/50], Loss: 15.2352\n",
      "Epoch [18/50], Loss: 12.9241\n",
      "Epoch [19/50], Loss: 16.9961\n",
      "Epoch [19/50], Loss: 13.3526\n",
      "Epoch [19/50], Loss: 15.0679\n",
      "Epoch [19/50], Loss: 16.6756\n",
      "Epoch [19/50], Loss: 20.6046\n",
      "Epoch [19/50], Loss: 16.7084\n",
      "Epoch [19/50], Loss: 15.1653\n",
      "Epoch [19/50], Loss: 21.9521\n",
      "Epoch [19/50], Loss: 18.0784\n",
      "Epoch [19/50], Loss: 15.2104\n",
      "Epoch [19/50], Loss: 13.2346\n",
      "Epoch [20/50], Loss: 16.9592\n",
      "Epoch [20/50], Loss: 13.2859\n",
      "Epoch [20/50], Loss: 15.0090\n",
      "Epoch [20/50], Loss: 16.6926\n",
      "Epoch [20/50], Loss: 20.5311\n",
      "Epoch [20/50], Loss: 16.6943\n",
      "Epoch [20/50], Loss: 15.2474\n",
      "Epoch [20/50], Loss: 21.8493\n",
      "Epoch [20/50], Loss: 17.9548\n",
      "Epoch [20/50], Loss: 15.1978\n",
      "Epoch [20/50], Loss: 13.4406\n",
      "Epoch [21/50], Loss: 16.9396\n",
      "Epoch [21/50], Loss: 13.2511\n",
      "Epoch [21/50], Loss: 14.9778\n",
      "Epoch [21/50], Loss: 16.7070\n",
      "Epoch [21/50], Loss: 20.4862\n",
      "Epoch [21/50], Loss: 16.6883\n",
      "Epoch [21/50], Loss: 15.3098\n",
      "Epoch [21/50], Loss: 21.7718\n",
      "Epoch [21/50], Loss: 17.8598\n",
      "Epoch [21/50], Loss: 15.1908\n",
      "Epoch [21/50], Loss: 13.5836\n",
      "Epoch [22/50], Loss: 16.9283\n",
      "Epoch [22/50], Loss: 13.2306\n",
      "Epoch [22/50], Loss: 14.9604\n",
      "Epoch [22/50], Loss: 16.7172\n",
      "Epoch [22/50], Loss: 20.4615\n",
      "Epoch [22/50], Loss: 16.6859\n",
      "Epoch [22/50], Loss: 15.3468\n",
      "Epoch [22/50], Loss: 21.7291\n",
      "Epoch [22/50], Loss: 17.8066\n",
      "Epoch [22/50], Loss: 15.1876\n",
      "Epoch [22/50], Loss: 13.6707\n",
      "Epoch [23/50], Loss: 16.9222\n",
      "Epoch [23/50], Loss: 13.2197\n",
      "Epoch [23/50], Loss: 14.9515\n",
      "Epoch [23/50], Loss: 16.7229\n",
      "Epoch [23/50], Loss: 20.4490\n",
      "Epoch [23/50], Loss: 16.6851\n",
      "Epoch [23/50], Loss: 15.3665\n",
      "Epoch [23/50], Loss: 21.7093\n",
      "Epoch [23/50], Loss: 17.7801\n",
      "Epoch [23/50], Loss: 15.1862\n",
      "Epoch [23/50], Loss: 13.7241\n",
      "Epoch [24/50], Loss: 16.9186\n",
      "Epoch [24/50], Loss: 13.2127\n",
      "Epoch [24/50], Loss: 14.9455\n",
      "Epoch [24/50], Loss: 16.7273\n",
      "Epoch [24/50], Loss: 20.4407\n",
      "Epoch [24/50], Loss: 16.6847\n",
      "Epoch [24/50], Loss: 15.3788\n",
      "Epoch [24/50], Loss: 21.6968\n",
      "Epoch [24/50], Loss: 17.7648\n",
      "Epoch [24/50], Loss: 15.1852\n",
      "Epoch [24/50], Loss: 13.7565\n",
      "Epoch [25/50], Loss: 16.9165\n",
      "Epoch [25/50], Loss: 13.2085\n",
      "Epoch [25/50], Loss: 14.9419\n",
      "Epoch [25/50], Loss: 16.7302\n",
      "Epoch [25/50], Loss: 20.4348\n",
      "Epoch [25/50], Loss: 16.6843\n",
      "Epoch [25/50], Loss: 15.3877\n",
      "Epoch [25/50], Loss: 21.6869\n",
      "Epoch [25/50], Loss: 17.7536\n",
      "Epoch [25/50], Loss: 15.1848\n",
      "Epoch [25/50], Loss: 13.7758\n",
      "Epoch [26/50], Loss: 16.9152\n",
      "Epoch [26/50], Loss: 13.2056\n",
      "Epoch [26/50], Loss: 14.9391\n",
      "Epoch [26/50], Loss: 16.7326\n",
      "Epoch [26/50], Loss: 20.4293\n",
      "Epoch [26/50], Loss: 16.6842\n",
      "Epoch [26/50], Loss: 15.3966\n",
      "Epoch [26/50], Loss: 21.6758\n",
      "Epoch [26/50], Loss: 17.7403\n",
      "Epoch [26/50], Loss: 15.1845\n",
      "Epoch [26/50], Loss: 13.7884\n",
      "Epoch [27/50], Loss: 16.9145\n",
      "Epoch [27/50], Loss: 13.2042\n",
      "Epoch [27/50], Loss: 14.9378\n",
      "Epoch [27/50], Loss: 16.7338\n",
      "Epoch [27/50], Loss: 20.4270\n",
      "Epoch [27/50], Loss: 16.6842\n",
      "Epoch [27/50], Loss: 15.4006\n",
      "Epoch [27/50], Loss: 21.6703\n",
      "Epoch [27/50], Loss: 17.7341\n",
      "Epoch [27/50], Loss: 15.1844\n",
      "Epoch [27/50], Loss: 13.7938\n",
      "Epoch [28/50], Loss: 16.9141\n",
      "Epoch [28/50], Loss: 13.2032\n",
      "Epoch [28/50], Loss: 14.9367\n",
      "Epoch [28/50], Loss: 16.7348\n",
      "Epoch [28/50], Loss: 20.4245\n",
      "Epoch [28/50], Loss: 16.6842\n",
      "Epoch [28/50], Loss: 15.4053\n",
      "Epoch [28/50], Loss: 21.6631\n",
      "Epoch [28/50], Loss: 17.7262\n",
      "Epoch [28/50], Loss: 15.1841\n",
      "Epoch [28/50], Loss: 13.7960\n",
      "Epoch [29/50], Loss: 16.9140\n",
      "Epoch [29/50], Loss: 13.2030\n",
      "Epoch [29/50], Loss: 14.9365\n",
      "Epoch [29/50], Loss: 16.7350\n",
      "Epoch [29/50], Loss: 20.4239\n",
      "Epoch [29/50], Loss: 16.6841\n",
      "Epoch [29/50], Loss: 15.4068\n",
      "Epoch [29/50], Loss: 21.6604\n",
      "Epoch [29/50], Loss: 17.7231\n",
      "Epoch [29/50], Loss: 15.1841\n",
      "Epoch [29/50], Loss: 13.7958\n",
      "Epoch [30/50], Loss: 16.9141\n",
      "Epoch [30/50], Loss: 13.2032\n",
      "Epoch [30/50], Loss: 14.9367\n",
      "Epoch [30/50], Loss: 16.7349\n",
      "Epoch [30/50], Loss: 20.4241\n",
      "Epoch [30/50], Loss: 16.6842\n",
      "Epoch [30/50], Loss: 15.4063\n",
      "Epoch [30/50], Loss: 21.6607\n",
      "Epoch [30/50], Loss: 17.7235\n",
      "Epoch [30/50], Loss: 15.1842\n",
      "Epoch [30/50], Loss: 13.7942\n",
      "Epoch [31/50], Loss: 16.9142\n",
      "Epoch [31/50], Loss: 13.2035\n",
      "Epoch [31/50], Loss: 14.9369\n",
      "Epoch [31/50], Loss: 16.7346\n",
      "Epoch [31/50], Loss: 20.4246\n",
      "Epoch [31/50], Loss: 16.6842\n",
      "Epoch [31/50], Loss: 15.4054\n",
      "Epoch [31/50], Loss: 21.6619\n",
      "Epoch [31/50], Loss: 17.7250\n",
      "Epoch [31/50], Loss: 15.1842\n",
      "Epoch [31/50], Loss: 13.7924\n",
      "Epoch [32/50], Loss: 16.9143\n",
      "Epoch [32/50], Loss: 13.2037\n",
      "Epoch [32/50], Loss: 14.9372\n",
      "Epoch [32/50], Loss: 16.7344\n",
      "Epoch [32/50], Loss: 20.4251\n",
      "Epoch [32/50], Loss: 16.6842\n",
      "Epoch [32/50], Loss: 15.4047\n",
      "Epoch [32/50], Loss: 21.6628\n",
      "Epoch [32/50], Loss: 17.7260\n",
      "Epoch [32/50], Loss: 15.1842\n",
      "Epoch [32/50], Loss: 13.7912\n",
      "Epoch [33/50], Loss: 16.9144\n",
      "Epoch [33/50], Loss: 13.2039\n",
      "Epoch [33/50], Loss: 14.9374\n",
      "Epoch [33/50], Loss: 16.7342\n",
      "Epoch [33/50], Loss: 20.4255\n",
      "Epoch [33/50], Loss: 16.6842\n",
      "Epoch [33/50], Loss: 15.4038\n",
      "Epoch [33/50], Loss: 21.6641\n",
      "Epoch [33/50], Loss: 17.7275\n",
      "Epoch [33/50], Loss: 15.1843\n",
      "Epoch [33/50], Loss: 13.7905\n",
      "Epoch [34/50], Loss: 16.9145\n",
      "Epoch [34/50], Loss: 13.2041\n",
      "Epoch [34/50], Loss: 14.9376\n",
      "Epoch [34/50], Loss: 16.7341\n",
      "Epoch [34/50], Loss: 20.4261\n",
      "Epoch [34/50], Loss: 16.6842\n",
      "Epoch [34/50], Loss: 15.4031\n",
      "Epoch [34/50], Loss: 21.6653\n",
      "Epoch [34/50], Loss: 17.7287\n",
      "Epoch [34/50], Loss: 15.1842\n",
      "Epoch [34/50], Loss: 13.7903\n",
      "Epoch [35/50], Loss: 16.9145\n",
      "Epoch [35/50], Loss: 13.2042\n",
      "Epoch [35/50], Loss: 14.9378\n",
      "Epoch [35/50], Loss: 16.7338\n",
      "Epoch [35/50], Loss: 20.4267\n",
      "Epoch [35/50], Loss: 16.6842\n",
      "Epoch [35/50], Loss: 15.4017\n",
      "Epoch [35/50], Loss: 21.6674\n",
      "Epoch [35/50], Loss: 17.7310\n",
      "Epoch [35/50], Loss: 15.1843\n",
      "Epoch [35/50], Loss: 13.7907\n",
      "Epoch [36/50], Loss: 16.9145\n",
      "Epoch [36/50], Loss: 13.2043\n",
      "Epoch [36/50], Loss: 14.9379\n",
      "Epoch [36/50], Loss: 16.7337\n",
      "Epoch [36/50], Loss: 20.4270\n",
      "Epoch [36/50], Loss: 16.6842\n",
      "Epoch [36/50], Loss: 15.4011\n",
      "Epoch [36/50], Loss: 21.6690\n",
      "Epoch [36/50], Loss: 17.7325\n",
      "Epoch [36/50], Loss: 15.1843\n",
      "Epoch [36/50], Loss: 13.7915\n",
      "Epoch [37/50], Loss: 16.9144\n",
      "Epoch [37/50], Loss: 13.2043\n",
      "Epoch [37/50], Loss: 14.9380\n",
      "Epoch [37/50], Loss: 16.7335\n",
      "Epoch [37/50], Loss: 20.4275\n",
      "Epoch [37/50], Loss: 16.6843\n",
      "Epoch [37/50], Loss: 15.4001\n",
      "Epoch [37/50], Loss: 21.6709\n",
      "Epoch [37/50], Loss: 17.7346\n",
      "Epoch [37/50], Loss: 15.1843\n",
      "Epoch [37/50], Loss: 13.7925\n",
      "Epoch [38/50], Loss: 16.9143\n",
      "Epoch [38/50], Loss: 13.2041\n",
      "Epoch [38/50], Loss: 14.9379\n",
      "Epoch [38/50], Loss: 16.7337\n",
      "Epoch [38/50], Loss: 20.4274\n",
      "Epoch [38/50], Loss: 16.6843\n",
      "Epoch [38/50], Loss: 15.4005\n",
      "Epoch [38/50], Loss: 21.6705\n",
      "Epoch [38/50], Loss: 17.7341\n",
      "Epoch [38/50], Loss: 15.1843\n",
      "Epoch [38/50], Loss: 13.7939\n",
      "Epoch [39/50], Loss: 16.9143\n",
      "Epoch [39/50], Loss: 13.2042\n",
      "Epoch [39/50], Loss: 14.9381\n",
      "Epoch [39/50], Loss: 16.7333\n",
      "Epoch [39/50], Loss: 20.4281\n",
      "Epoch [39/50], Loss: 16.6842\n",
      "Epoch [39/50], Loss: 15.3992\n",
      "Epoch [39/50], Loss: 21.6731\n",
      "Epoch [39/50], Loss: 17.7366\n",
      "Epoch [39/50], Loss: 15.1842\n",
      "Epoch [39/50], Loss: 13.7956\n",
      "Epoch [40/50], Loss: 16.9143\n",
      "Epoch [40/50], Loss: 13.2045\n",
      "Epoch [40/50], Loss: 14.9386\n",
      "Epoch [40/50], Loss: 16.7327\n",
      "Epoch [40/50], Loss: 20.4299\n",
      "Epoch [40/50], Loss: 16.6843\n",
      "Epoch [40/50], Loss: 15.3964\n",
      "Epoch [40/50], Loss: 21.6783\n",
      "Epoch [40/50], Loss: 17.7415\n",
      "Epoch [40/50], Loss: 15.1843\n",
      "Epoch [40/50], Loss: 13.7982\n",
      "Epoch [41/50], Loss: 16.9141\n",
      "Epoch [41/50], Loss: 13.2044\n",
      "Epoch [41/50], Loss: 14.9389\n",
      "Epoch [41/50], Loss: 16.7324\n",
      "Epoch [41/50], Loss: 20.4315\n",
      "Epoch [41/50], Loss: 16.6841\n",
      "Epoch [41/50], Loss: 15.3927\n",
      "Epoch [41/50], Loss: 21.6850\n",
      "Epoch [41/50], Loss: 17.7493\n",
      "Epoch [41/50], Loss: 15.1844\n",
      "Epoch [41/50], Loss: 13.8001\n",
      "Epoch [42/50], Loss: 16.9138\n",
      "Epoch [42/50], Loss: 13.2030\n",
      "Epoch [42/50], Loss: 14.9371\n",
      "Epoch [42/50], Loss: 16.7340\n",
      "Epoch [42/50], Loss: 20.4273\n",
      "Epoch [42/50], Loss: 16.6842\n",
      "Epoch [42/50], Loss: 15.3993\n",
      "Epoch [42/50], Loss: 21.6751\n",
      "Epoch [42/50], Loss: 17.7393\n",
      "Epoch [42/50], Loss: 15.1843\n",
      "Epoch [42/50], Loss: 13.8018\n",
      "Epoch [43/50], Loss: 16.9137\n",
      "Epoch [43/50], Loss: 13.2024\n",
      "Epoch [43/50], Loss: 14.9361\n",
      "Epoch [43/50], Loss: 16.7353\n",
      "Epoch [43/50], Loss: 20.4238\n",
      "Epoch [43/50], Loss: 16.6841\n",
      "Epoch [43/50], Loss: 15.4065\n",
      "Epoch [43/50], Loss: 21.6629\n",
      "Epoch [43/50], Loss: 17.7255\n",
      "Epoch [43/50], Loss: 15.1840\n",
      "Epoch [43/50], Loss: 13.8042\n",
      "Epoch [44/50], Loss: 16.9137\n",
      "Epoch [44/50], Loss: 13.2025\n",
      "Epoch [44/50], Loss: 14.9362\n",
      "Epoch [44/50], Loss: 16.7352\n",
      "Epoch [44/50], Loss: 20.4238\n",
      "Epoch [44/50], Loss: 16.6842\n",
      "Epoch [44/50], Loss: 15.4070\n",
      "Epoch [44/50], Loss: 21.6614\n",
      "Epoch [44/50], Loss: 17.7236\n",
      "Epoch [44/50], Loss: 15.1840\n",
      "Epoch [44/50], Loss: 13.8030\n",
      "Epoch [45/50], Loss: 16.9137\n",
      "Epoch [45/50], Loss: 13.2026\n",
      "Epoch [45/50], Loss: 14.9363\n",
      "Epoch [45/50], Loss: 16.7351\n",
      "Epoch [45/50], Loss: 20.4240\n",
      "Epoch [45/50], Loss: 16.6842\n",
      "Epoch [45/50], Loss: 15.4064\n",
      "Epoch [45/50], Loss: 21.6618\n",
      "Epoch [45/50], Loss: 17.7244\n",
      "Epoch [45/50], Loss: 15.1841\n",
      "Epoch [45/50], Loss: 13.8000\n",
      "Epoch [46/50], Loss: 16.9139\n",
      "Epoch [46/50], Loss: 13.2030\n",
      "Epoch [46/50], Loss: 14.9366\n",
      "Epoch [46/50], Loss: 16.7349\n",
      "Epoch [46/50], Loss: 20.4242\n",
      "Epoch [46/50], Loss: 16.6842\n",
      "Epoch [46/50], Loss: 15.4060\n",
      "Epoch [46/50], Loss: 21.6618\n",
      "Epoch [46/50], Loss: 17.7247\n",
      "Epoch [46/50], Loss: 15.1841\n",
      "Epoch [46/50], Loss: 13.7974\n",
      "Epoch [47/50], Loss: 16.9141\n",
      "Epoch [47/50], Loss: 13.2033\n",
      "Epoch [47/50], Loss: 14.9368\n",
      "Epoch [47/50], Loss: 16.7347\n",
      "Epoch [47/50], Loss: 20.4246\n",
      "Epoch [47/50], Loss: 16.6842\n",
      "Epoch [47/50], Loss: 15.4054\n",
      "Epoch [47/50], Loss: 21.6622\n",
      "Epoch [47/50], Loss: 17.7253\n",
      "Epoch [47/50], Loss: 15.1842\n",
      "Epoch [47/50], Loss: 13.7951\n",
      "Epoch [48/50], Loss: 16.9142\n",
      "Epoch [48/50], Loss: 13.2036\n",
      "Epoch [48/50], Loss: 14.9371\n",
      "Epoch [48/50], Loss: 16.7345\n",
      "Epoch [48/50], Loss: 20.4250\n",
      "Epoch [48/50], Loss: 16.6842\n",
      "Epoch [48/50], Loss: 15.4048\n",
      "Epoch [48/50], Loss: 21.6628\n",
      "Epoch [48/50], Loss: 17.7260\n",
      "Epoch [48/50], Loss: 15.1842\n",
      "Epoch [48/50], Loss: 13.7935\n",
      "Epoch [49/50], Loss: 16.9143\n",
      "Epoch [49/50], Loss: 13.2039\n",
      "Epoch [49/50], Loss: 14.9373\n",
      "Epoch [49/50], Loss: 16.7343\n",
      "Epoch [49/50], Loss: 20.4254\n",
      "Epoch [49/50], Loss: 16.6842\n",
      "Epoch [49/50], Loss: 15.4042\n",
      "Epoch [49/50], Loss: 21.6637\n",
      "Epoch [49/50], Loss: 17.7269\n",
      "Epoch [49/50], Loss: 15.1842\n",
      "Epoch [49/50], Loss: 13.7925\n",
      "Epoch [50/50], Loss: 16.9144\n",
      "Epoch [50/50], Loss: 13.2041\n",
      "Epoch [50/50], Loss: 14.9376\n",
      "Epoch [50/50], Loss: 16.7340\n",
      "Epoch [50/50], Loss: 20.4260\n",
      "Epoch [50/50], Loss: 16.6842\n",
      "Epoch [50/50], Loss: 15.4031\n",
      "Epoch [50/50], Loss: 21.6653\n",
      "Epoch [50/50], Loss: 17.7288\n",
      "Epoch [50/50], Loss: 15.1842\n",
      "Epoch [50/50], Loss: 13.7920\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Unpack the batch\n",
    "        modalities, labels = batch\n",
    "        modality_1 = modalities[0]  # Each has shape (batch_size, 700, 1)\n",
    "\n",
    "        # Reshape or project the input to match model requirements\n",
    "        modality_1 = modality_1.view(modality_1.size(0), -1)  # Flatten to [batch_size, 700]\n",
    "\n",
    "        # Ensure the input matches the expected size [batch_size, 768]\n",
    "        if modality_1.size(1) > 768:\n",
    "            modality_1 = modality_1[:, :768]  # Truncate\n",
    "        elif modality_1.size(1) < 768:\n",
    "            padding = torch.zeros((modality_1.size(0), 768 - modality_1.size(1)), dtype=torch.float32).to(modality_1.device)\n",
    "            modality_1 = torch.cat((modality_1, padding), dim=1)  # Pad\n",
    "\n",
    "        # Convert modality and labels to float32\n",
    "        modality_1 = modality_1.float()\n",
    "        labels = labels.float()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = [modality_1]\n",
    "        output = model(inputs)\n",
    "\n",
    "        # Reshape labels to match the output shape\n",
    "        labels = labels.view(output.shape)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, labels)\n",
    "        RMSE_loss = torch.sqrt(loss)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        RMSE_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {RMSE_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = loaders.MultiModalityDataset(features_csv[features_csv[\"split\"]==\"dev\"], \n",
    "                                            modalities = {\"video\"}, label = \"PTSD_severity\")\n",
    "dev_dataloader = DataLoader(dev_loader, batch_size=16, collate_fn=loaders.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "[[34.10594 ]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.10594 ]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.10594 ]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.10594 ]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]\n",
      " [34.105938]]\n"
     ]
    }
   ],
   "source": [
    "# Model prediction\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch in dev_dataloader:\n",
    "        modalities, _ = batch  # Ignore labels if unavailable\n",
    "        modality_1 = modalities[0]\n",
    "        \n",
    "        # Preprocess the input to match model requirements\n",
    "        modality_1 = modality_1.view(modality_1.size(0), -1)  # Flatten to [batch_size, 700]\n",
    "\n",
    "        # Ensure the input matches the expected size [batch_size, 768]\n",
    "        if modality_1.size(1) > 768:\n",
    "            modality_1 = modality_1[:, :768]  # Truncate\n",
    "        elif modality_1.size(1) < 768:\n",
    "            padding = torch.zeros((modality_1.size(0), 768 - modality_1.size(1)), dtype=torch.float32).to(modality_1.device)\n",
    "            modality_1 = torch.cat((modality_1, padding), dim=1)  # Pad\n",
    "\n",
    "        # Ensure consistent data type\n",
    "        modality_1 = modality_1.float()  # Convert to float32\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = [modality_1]\n",
    "        output = model(inputs)  # Model's prediction\n",
    "\n",
    "        # Collect predictions\n",
    "        predictions.append(output)\n",
    "\n",
    "# Combine predictions into a single tensor\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "y_pred = predictions.cpu().numpy()  # Convert to NumPy for further processing\n",
    "print(f\"Predictions:\\n{y_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(features_csv[features_csv[\"split\"]==\"dev\"][\"PTSD_severity\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_csv['col_encoded'] = features_csv['gender'].map({'female': 0, 'male': 1, None:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1., nan])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(features_csv['col_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of gender_test: (56,)\n",
      "Shape of y_test: (56,)\n",
      "Shape of y_pred: (56, 1)\n"
     ]
    }
   ],
   "source": [
    "gender_train, gender_test = features_csv[features_csv[\"split\"]==\"train\"][\"col_encoded\"], features_csv[features_csv[\"split\"]==\"dev\"][\"col_encoded\"]\n",
    "\n",
    "# Ensure that `gender_test`, `y_test`, and `y_pred` are aligned\n",
    "print(f\"Shape of gender_test: {gender_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "print(f\"Shape of y_pred: {y_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Parity Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference\n",
    "# Calculate Demographic Parity Difference\n",
    "demographic_parity = demographic_parity_difference(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred[:,0],\n",
    "    sensitive_features=gender_test\n",
    ")\n",
    "\n",
    "print(f\"Demographic Parity Difference: {demographic_parity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
