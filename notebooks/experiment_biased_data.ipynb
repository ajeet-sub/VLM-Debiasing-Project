{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/hice1/mbibars3/scratch/vlm-debiasing/VLM-Debiasing-Project/scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import model as m\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import loaders\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-Audio-Visual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- text: Roberta\n",
    "- audio: AST\n",
    "- visual: densenet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>PTSD_severity</th>\n",
       "      <th>PTSD_label</th>\n",
       "      <th>gender</th>\n",
       "      <th>audio</th>\n",
       "      <th>text</th>\n",
       "      <th>visual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dev</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "      <td>/home/hice1/mbibars3/scratch/vlm-debiasing/dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split  PTSD_severity  PTSD_label  gender  \\\n",
       "0  train           22.0           0  female   \n",
       "1   test           23.0           0    male   \n",
       "2  train           19.0           0    male   \n",
       "3  train           67.0           1  female   \n",
       "4    dev           39.0           0    male   \n",
       "\n",
       "                                               audio  \\\n",
       "0  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "1  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "2  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "3  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "4  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "\n",
       "                                                text  \\\n",
       "0  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "1  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "2  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "3  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "4  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...   \n",
       "\n",
       "                                              visual  \n",
       "0  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...  \n",
       "1  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...  \n",
       "2  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...  \n",
       "3  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...  \n",
       "4  /home/hice1/mbibars3/scratch/vlm-debiasing/dat...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_csv = pd.read_csv(\"/home/hice1/mbibars3/scratch/vlm-debiasing/data/e-daic/audio_text_visual_paths.csv\")\n",
    "features_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality 1 shape: torch.Size([16, 768, 1])\n",
      "Labels shape: tensor([22., 19., 67., 17., 25., 25., 50., 34., 19., 26., 28., 44., 61., 21.,\n",
      "        67., 36.])\n"
     ]
    }
   ],
   "source": [
    "train_loader = loaders.MultiModalityDataset(features_csv[features_csv[\"split\"]==\"train\"], \n",
    "                                            modalities = {\"audio\"}, label = \"PTSD_severity\")\n",
    "dataloader = DataLoader(train_loader, batch_size=16, collate_fn=loaders.collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    modalities, labels = batch\n",
    "    print(f\"Modality 1 shape: {modalities[0].shape}\")  # Expected shape: (batch_size, feature, 1)\n",
    "    #print(f\"Modality 2 shape: {modalities[1].shape}\")\n",
    "    #print(f\"Modality 3 shape: {modalities[2].shape}\")\n",
    "    print(f\"Labels shape: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input dimensions for each modality\n",
    "input_dims = [768]  # These are the feature dimensions for each modality\n",
    "\n",
    "# Initialize MultiModalPerceiver model\n",
    "model = m.MultiModalPerceiver(\n",
    "    input_dims=input_dims,\n",
    "    input_channels=1,\n",
    "    input_axis=1,\n",
    "    projection_dim=256,\n",
    "    num_latents=16,\n",
    "    latent_dim=128,\n",
    "    depth=8,\n",
    "    cross_heads=8,\n",
    "    latent_heads=8,\n",
    "    cross_dim_head=32,\n",
    "    latent_dim_head=32,\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.0,\n",
    "    output_dim=1,\n",
    "    weight_tie_layers=True,\n",
    "    fourier_encode_data=False,\n",
    "    max_freq=10,\n",
    "    num_freq_bands=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 39.2518\n",
      "Epoch [1/50], Loss: 34.8764\n",
      "Epoch [1/50], Loss: 34.2645\n",
      "Epoch [1/50], Loss: 31.5906\n",
      "Epoch [1/50], Loss: 37.2623\n",
      "Epoch [1/50], Loss: 32.2111\n",
      "Epoch [1/50], Loss: 26.6679\n",
      "Epoch [1/50], Loss: 40.4853\n",
      "Epoch [1/50], Loss: 38.8633\n",
      "Epoch [1/50], Loss: 31.2483\n",
      "Epoch [1/50], Loss: 15.6415\n",
      "Epoch [2/50], Loss: 32.4840\n",
      "Epoch [2/50], Loss: 31.0815\n",
      "Epoch [2/50], Loss: 31.7376\n",
      "Epoch [2/50], Loss: 29.9207\n",
      "Epoch [2/50], Loss: 35.9818\n",
      "Epoch [2/50], Loss: 31.0198\n",
      "Epoch [2/50], Loss: 25.5247\n",
      "Epoch [2/50], Loss: 39.2864\n",
      "Epoch [2/50], Loss: 37.5775\n",
      "Epoch [2/50], Loss: 30.0137\n",
      "Epoch [2/50], Loss: 14.3279\n",
      "Epoch [3/50], Loss: 31.2709\n",
      "Epoch [3/50], Loss: 29.7913\n",
      "Epoch [3/50], Loss: 30.4817\n",
      "Epoch [3/50], Loss: 28.7427\n",
      "Epoch [3/50], Loss: 34.7999\n",
      "Epoch [3/50], Loss: 29.8129\n",
      "Epoch [3/50], Loss: 24.3454\n",
      "Epoch [3/50], Loss: 38.0369\n",
      "Epoch [3/50], Loss: 36.2372\n",
      "Epoch [3/50], Loss: 28.7366\n",
      "Epoch [3/50], Loss: 12.9709\n",
      "Epoch [4/50], Loss: 30.0034\n",
      "Epoch [4/50], Loss: 28.4238\n",
      "Epoch [4/50], Loss: 29.1433\n",
      "Epoch [4/50], Loss: 27.4903\n",
      "Epoch [4/50], Loss: 33.5337\n",
      "Epoch [4/50], Loss: 28.5199\n",
      "Epoch [4/50], Loss: 23.0933\n",
      "Epoch [4/50], Loss: 36.6927\n",
      "Epoch [4/50], Loss: 34.7920\n",
      "Epoch [4/50], Loss: 27.3758\n",
      "Epoch [4/50], Loss: 11.5439\n",
      "Epoch [5/50], Loss: 28.6620\n",
      "Epoch [5/50], Loss: 26.9684\n",
      "Epoch [5/50], Loss: 27.7241\n",
      "Epoch [5/50], Loss: 26.1763\n",
      "Epoch [5/50], Loss: 32.1974\n",
      "Epoch [5/50], Loss: 27.1581\n",
      "Epoch [5/50], Loss: 21.7906\n",
      "Epoch [5/50], Loss: 35.2650\n",
      "Epoch [5/50], Loss: 33.2484\n",
      "Epoch [5/50], Loss: 25.9400\n",
      "Epoch [5/50], Loss: 10.0733\n",
      "Epoch [6/50], Loss: 27.2523\n",
      "Epoch [6/50], Loss: 25.4276\n",
      "Epoch [6/50], Loss: 26.2293\n",
      "Epoch [6/50], Loss: 24.8127\n",
      "Epoch [6/50], Loss: 30.8013\n",
      "Epoch [6/50], Loss: 25.7425\n",
      "Epoch [6/50], Loss: 20.4619\n",
      "Epoch [6/50], Loss: 33.7670\n",
      "Epoch [6/50], Loss: 31.6187\n",
      "Epoch [6/50], Loss: 24.4492\n",
      "Epoch [6/50], Loss: 8.6215\n",
      "Epoch [7/50], Loss: 25.7959\n",
      "Epoch [7/50], Loss: 23.8205\n",
      "Epoch [7/50], Loss: 24.6795\n",
      "Epoch [7/50], Loss: 23.4262\n",
      "Epoch [7/50], Loss: 29.3666\n",
      "Epoch [7/50], Loss: 24.2984\n",
      "Epoch [7/50], Loss: 19.1438\n",
      "Epoch [7/50], Loss: 32.2188\n",
      "Epoch [7/50], Loss: 29.9223\n",
      "Epoch [7/50], Loss: 22.9331\n",
      "Epoch [7/50], Loss: 7.3029\n",
      "Epoch [8/50], Loss: 24.3247\n",
      "Epoch [8/50], Loss: 22.1786\n",
      "Epoch [8/50], Loss: 23.1086\n",
      "Epoch [8/50], Loss: 22.0571\n",
      "Epoch [8/50], Loss: 27.9271\n",
      "Epoch [8/50], Loss: 22.8660\n",
      "Epoch [8/50], Loss: 17.8901\n",
      "Epoch [8/50], Loss: 30.6540\n",
      "Epoch [8/50], Loss: 28.1930\n",
      "Epoch [8/50], Loss: 21.4361\n",
      "Epoch [8/50], Loss: 6.3146\n",
      "Epoch [9/50], Loss: 22.8844\n",
      "Epoch [9/50], Loss: 20.5495\n",
      "Epoch [9/50], Loss: 21.5659\n",
      "Epoch [9/50], Loss: 20.7598\n",
      "Epoch [9/50], Loss: 26.5301\n",
      "Epoch [9/50], Loss: 21.5001\n",
      "Epoch [9/50], Loss: 16.7692\n",
      "Epoch [9/50], Loss: 29.1208\n",
      "Epoch [9/50], Loss: 26.4820\n",
      "Epoch [9/50], Loss: 20.0190\n",
      "Epoch [9/50], Loss: 5.9085\n",
      "Epoch [10/50], Loss: 21.5349\n",
      "Epoch [10/50], Loss: 18.9981\n",
      "Epoch [10/50], Loss: 20.1158\n",
      "Epoch [10/50], Loss: 19.5980\n",
      "Epoch [10/50], Loss: 25.2328\n",
      "Epoch [10/50], Loss: 20.2648\n",
      "Epoch [10/50], Loss: 15.8521\n",
      "Epoch [10/50], Loss: 27.6797\n",
      "Epoch [10/50], Loss: 24.8560\n",
      "Epoch [10/50], Loss: 18.7519\n",
      "Epoch [10/50], Loss: 6.2029\n",
      "Epoch [11/50], Loss: 20.3401\n",
      "Epoch [11/50], Loss: 17.5969\n",
      "Epoch [11/50], Loss: 18.8250\n",
      "Epoch [11/50], Loss: 18.6280\n",
      "Epoch [11/50], Loss: 24.0885\n",
      "Epoch [11/50], Loss: 19.2162\n",
      "Epoch [11/50], Loss: 15.1868\n",
      "Epoch [11/50], Loss: 26.3863\n",
      "Epoch [11/50], Loss: 23.3786\n",
      "Epoch [11/50], Loss: 17.6905\n",
      "Epoch [11/50], Loss: 7.0291\n",
      "Epoch [12/50], Loss: 19.3463\n",
      "Epoch [12/50], Loss: 16.4028\n",
      "Epoch [12/50], Loss: 17.7411\n",
      "Epoch [12/50], Loss: 17.8787\n",
      "Epoch [12/50], Loss: 23.1294\n",
      "Epoch [12/50], Loss: 18.3823\n",
      "Epoch [12/50], Loss: 14.7785\n",
      "Epoch [12/50], Loss: 25.2734\n",
      "Epoch [12/50], Loss: 22.0898\n",
      "Epoch [12/50], Loss: 16.8571\n",
      "Epoch [12/50], Loss: 8.1057\n",
      "Epoch [13/50], Loss: 18.5677\n",
      "Epoch [13/50], Loss: 15.4411\n",
      "Epoch [13/50], Loss: 16.8807\n",
      "Epoch [13/50], Loss: 17.3459\n",
      "Epoch [13/50], Loss: 22.3618\n",
      "Epoch [13/50], Loss: 17.7613\n",
      "Epoch [13/50], Loss: 14.5935\n",
      "Epoch [13/50], Loss: 24.3513\n",
      "Epoch [13/50], Loss: 21.0061\n",
      "Epoch [13/50], Loss: 16.2435\n",
      "Epoch [13/50], Loss: 9.2231\n",
      "Epoch [14/50], Loss: 17.9921\n",
      "Epoch [14/50], Loss: 14.7092\n",
      "Epoch [14/50], Loss: 16.2345\n",
      "Epoch [14/50], Loss: 17.0010\n",
      "Epoch [14/50], Loss: 21.7739\n",
      "Epoch [14/50], Loss: 17.3290\n",
      "Epoch [14/50], Loss: 14.5751\n",
      "Epoch [14/50], Loss: 23.6147\n",
      "Epoch [14/50], Loss: 20.1276\n",
      "Epoch [14/50], Loss: 15.8207\n",
      "Epoch [14/50], Loss: 10.2597\n",
      "Epoch [15/50], Loss: 17.5919\n",
      "Epoch [15/50], Loss: 14.1846\n",
      "Epoch [15/50], Loss: 15.7761\n",
      "Epoch [15/50], Loss: 16.8025\n",
      "Epoch [15/50], Loss: 21.3427\n",
      "Epoch [15/50], Loss: 17.0489\n",
      "Epoch [15/50], Loss: 14.6601\n",
      "Epoch [15/50], Loss: 23.0493\n",
      "Epoch [15/50], Loss: 19.4424\n",
      "Epoch [15/50], Loss: 15.5485\n",
      "Epoch [15/50], Loss: 11.1525\n",
      "Epoch [16/50], Loss: 17.3316\n",
      "Epoch [16/50], Loss: 13.8374\n",
      "Epoch [16/50], Loss: 15.4746\n",
      "Epoch [16/50], Loss: 16.7061\n",
      "Epoch [16/50], Loss: 21.0367\n",
      "Epoch [16/50], Loss: 16.8792\n",
      "Epoch [16/50], Loss: 14.7923\n",
      "Epoch [16/50], Loss: 22.6364\n",
      "Epoch [16/50], Loss: 18.9358\n",
      "Epoch [16/50], Loss: 15.3849\n",
      "Epoch [16/50], Loss: 11.8808\n",
      "Epoch [17/50], Loss: 17.1693\n",
      "Epoch [17/50], Loss: 13.6181\n",
      "Epoch [17/50], Loss: 15.2879\n",
      "Epoch [17/50], Loss: 16.6715\n",
      "Epoch [17/50], Loss: 20.8336\n",
      "Epoch [17/50], Loss: 16.7839\n",
      "Epoch [17/50], Loss: 14.9348\n",
      "Epoch [17/50], Loss: 22.3313\n",
      "Epoch [17/50], Loss: 18.5580\n",
      "Epoch [17/50], Loss: 15.2909\n",
      "Epoch [17/50], Loss: 12.4444\n",
      "Epoch [18/50], Loss: 17.0743\n",
      "Epoch [18/50], Loss: 13.4895\n",
      "Epoch [18/50], Loss: 15.1820\n",
      "Epoch [18/50], Loss: 16.6659\n",
      "Epoch [18/50], Loss: 20.7086\n",
      "Epoch [18/50], Loss: 16.7348\n",
      "Epoch [18/50], Loss: 15.0631\n",
      "Epoch [18/50], Loss: 22.1021\n",
      "Epoch [18/50], Loss: 18.2763\n",
      "Epoch [18/50], Loss: 15.2390\n",
      "Epoch [18/50], Loss: 12.8855\n",
      "Epoch [19/50], Loss: 17.0026\n",
      "Epoch [19/50], Loss: 13.3657\n",
      "Epoch [19/50], Loss: 15.0790\n",
      "Epoch [19/50], Loss: 16.6741\n",
      "Epoch [19/50], Loss: 20.6131\n",
      "Epoch [19/50], Loss: 16.7096\n",
      "Epoch [19/50], Loss: 15.1618\n",
      "Epoch [19/50], Loss: 21.9532\n",
      "Epoch [19/50], Loss: 18.0806\n",
      "Epoch [19/50], Loss: 15.2113\n",
      "Epoch [19/50], Loss: 13.2112\n",
      "Epoch [20/50], Loss: 16.9627\n",
      "Epoch [20/50], Loss: 13.2947\n",
      "Epoch [20/50], Loss: 15.0183\n",
      "Epoch [20/50], Loss: 16.6892\n",
      "Epoch [20/50], Loss: 20.5435\n",
      "Epoch [20/50], Loss: 16.6961\n",
      "Epoch [20/50], Loss: 15.2353\n",
      "Epoch [20/50], Loss: 21.8645\n",
      "Epoch [20/50], Loss: 17.9700\n",
      "Epoch [20/50], Loss: 15.1988\n",
      "Epoch [20/50], Loss: 13.4239\n",
      "Epoch [21/50], Loss: 16.9416\n",
      "Epoch [21/50], Loss: 13.2567\n",
      "Epoch [21/50], Loss: 14.9837\n",
      "Epoch [21/50], Loss: 16.7036\n",
      "Epoch [21/50], Loss: 20.4967\n",
      "Epoch [21/50], Loss: 16.6894\n",
      "Epoch [21/50], Loss: 15.2981\n",
      "Epoch [21/50], Loss: 21.7889\n",
      "Epoch [21/50], Loss: 17.8758\n",
      "Epoch [21/50], Loss: 15.1912\n",
      "Epoch [21/50], Loss: 13.5785\n",
      "Epoch [22/50], Loss: 16.9291\n",
      "Epoch [22/50], Loss: 13.2342\n",
      "Epoch [22/50], Loss: 14.9652\n",
      "Epoch [22/50], Loss: 16.7136\n",
      "Epoch [22/50], Loss: 20.4728\n",
      "Epoch [22/50], Loss: 16.6868\n",
      "Epoch [22/50], Loss: 15.3309\n",
      "Epoch [22/50], Loss: 21.7550\n",
      "Epoch [22/50], Loss: 17.8319\n",
      "Epoch [22/50], Loss: 15.1880\n",
      "Epoch [22/50], Loss: 13.6713\n",
      "Epoch [23/50], Loss: 16.9223\n",
      "Epoch [23/50], Loss: 13.2206\n",
      "Epoch [23/50], Loss: 14.9531\n",
      "Epoch [23/50], Loss: 16.7214\n",
      "Epoch [23/50], Loss: 20.4538\n",
      "Epoch [23/50], Loss: 16.6854\n",
      "Epoch [23/50], Loss: 15.3584\n",
      "Epoch [23/50], Loss: 21.7237\n",
      "Epoch [23/50], Loss: 17.7959\n",
      "Epoch [23/50], Loss: 15.1861\n",
      "Epoch [23/50], Loss: 13.7288\n",
      "Epoch [24/50], Loss: 16.9186\n",
      "Epoch [24/50], Loss: 13.2138\n",
      "Epoch [24/50], Loss: 14.9473\n",
      "Epoch [24/50], Loss: 16.7259\n",
      "Epoch [24/50], Loss: 20.4447\n",
      "Epoch [24/50], Loss: 16.6849\n",
      "Epoch [24/50], Loss: 15.3758\n",
      "Epoch [24/50], Loss: 21.7044\n",
      "Epoch [24/50], Loss: 17.7689\n",
      "Epoch [24/50], Loss: 15.1850\n",
      "Epoch [24/50], Loss: 13.7729\n",
      "Epoch [25/50], Loss: 16.9158\n",
      "Epoch [25/50], Loss: 13.2084\n",
      "Epoch [25/50], Loss: 14.9430\n",
      "Epoch [25/50], Loss: 16.7285\n",
      "Epoch [25/50], Loss: 20.4402\n",
      "Epoch [25/50], Loss: 16.6847\n",
      "Epoch [25/50], Loss: 15.3774\n",
      "Epoch [25/50], Loss: 21.7055\n",
      "Epoch [25/50], Loss: 17.7741\n",
      "Epoch [25/50], Loss: 15.1850\n",
      "Epoch [25/50], Loss: 13.7817\n",
      "Epoch [26/50], Loss: 16.9147\n",
      "Epoch [26/50], Loss: 13.2041\n",
      "Epoch [26/50], Loss: 14.9373\n",
      "Epoch [26/50], Loss: 16.7346\n",
      "Epoch [26/50], Loss: 20.4248\n",
      "Epoch [26/50], Loss: 16.6842\n",
      "Epoch [26/50], Loss: 15.4061\n",
      "Epoch [26/50], Loss: 21.6632\n",
      "Epoch [26/50], Loss: 17.7250\n",
      "Epoch [26/50], Loss: 15.1840\n",
      "Epoch [26/50], Loss: 13.8080\n",
      "Epoch [27/50], Loss: 16.9133\n",
      "Epoch [27/50], Loss: 13.2017\n",
      "Epoch [27/50], Loss: 14.9354\n",
      "Epoch [27/50], Loss: 16.7359\n",
      "Epoch [27/50], Loss: 20.4224\n",
      "Epoch [27/50], Loss: 16.6841\n",
      "Epoch [27/50], Loss: 15.4093\n",
      "Epoch [27/50], Loss: 21.6589\n",
      "Epoch [27/50], Loss: 17.7204\n",
      "Epoch [27/50], Loss: 15.1839\n",
      "Epoch [27/50], Loss: 13.8102\n",
      "Epoch [28/50], Loss: 16.9132\n",
      "Epoch [28/50], Loss: 13.2013\n",
      "Epoch [28/50], Loss: 14.9349\n",
      "Epoch [28/50], Loss: 16.7365\n",
      "Epoch [28/50], Loss: 20.4211\n",
      "Epoch [28/50], Loss: 16.6842\n",
      "Epoch [28/50], Loss: 15.4117\n",
      "Epoch [28/50], Loss: 21.6550\n",
      "Epoch [28/50], Loss: 17.7162\n",
      "Epoch [28/50], Loss: 15.1839\n",
      "Epoch [28/50], Loss: 13.8081\n",
      "Epoch [29/50], Loss: 16.9133\n",
      "Epoch [29/50], Loss: 13.2017\n",
      "Epoch [29/50], Loss: 14.9353\n",
      "Epoch [29/50], Loss: 16.7361\n",
      "Epoch [29/50], Loss: 20.4217\n",
      "Epoch [29/50], Loss: 16.6842\n",
      "Epoch [29/50], Loss: 15.4105\n",
      "Epoch [29/50], Loss: 21.6560\n",
      "Epoch [29/50], Loss: 17.7176\n",
      "Epoch [29/50], Loss: 15.1840\n",
      "Epoch [29/50], Loss: 13.8032\n",
      "Epoch [30/50], Loss: 16.9137\n",
      "Epoch [30/50], Loss: 13.2023\n",
      "Epoch [30/50], Loss: 14.9359\n",
      "Epoch [30/50], Loss: 16.7356\n",
      "Epoch [30/50], Loss: 20.4226\n",
      "Epoch [30/50], Loss: 16.6841\n",
      "Epoch [30/50], Loss: 15.4089\n",
      "Epoch [30/50], Loss: 21.6576\n",
      "Epoch [30/50], Loss: 17.7198\n",
      "Epoch [30/50], Loss: 15.1840\n",
      "Epoch [30/50], Loss: 13.7985\n",
      "Epoch [31/50], Loss: 16.9140\n",
      "Epoch [31/50], Loss: 13.2029\n",
      "Epoch [31/50], Loss: 14.9364\n",
      "Epoch [31/50], Loss: 16.7351\n",
      "Epoch [31/50], Loss: 20.4236\n",
      "Epoch [31/50], Loss: 16.6842\n",
      "Epoch [31/50], Loss: 15.4073\n",
      "Epoch [31/50], Loss: 21.6593\n",
      "Epoch [31/50], Loss: 17.7219\n",
      "Epoch [31/50], Loss: 15.1841\n",
      "Epoch [31/50], Loss: 13.7946\n",
      "Epoch [32/50], Loss: 16.9142\n",
      "Epoch [32/50], Loss: 13.2034\n",
      "Epoch [32/50], Loss: 14.9368\n",
      "Epoch [32/50], Loss: 16.7348\n",
      "Epoch [32/50], Loss: 20.4242\n",
      "Epoch [32/50], Loss: 16.6842\n",
      "Epoch [32/50], Loss: 15.4061\n",
      "Epoch [32/50], Loss: 21.6605\n",
      "Epoch [32/50], Loss: 17.7235\n",
      "Epoch [32/50], Loss: 15.1842\n",
      "Epoch [32/50], Loss: 13.7920\n",
      "Epoch [33/50], Loss: 16.9144\n",
      "Epoch [33/50], Loss: 13.2038\n",
      "Epoch [33/50], Loss: 14.9371\n",
      "Epoch [33/50], Loss: 16.7345\n",
      "Epoch [33/50], Loss: 20.4247\n",
      "Epoch [33/50], Loss: 16.6842\n",
      "Epoch [33/50], Loss: 15.4053\n",
      "Epoch [33/50], Loss: 21.6614\n",
      "Epoch [33/50], Loss: 17.7247\n",
      "Epoch [33/50], Loss: 15.1842\n",
      "Epoch [33/50], Loss: 13.7902\n",
      "Epoch [34/50], Loss: 16.9145\n",
      "Epoch [34/50], Loss: 13.2040\n",
      "Epoch [34/50], Loss: 14.9373\n",
      "Epoch [34/50], Loss: 16.7343\n",
      "Epoch [34/50], Loss: 20.4251\n",
      "Epoch [34/50], Loss: 16.6842\n",
      "Epoch [34/50], Loss: 15.4047\n",
      "Epoch [34/50], Loss: 21.6621\n",
      "Epoch [34/50], Loss: 17.7254\n",
      "Epoch [34/50], Loss: 15.1843\n",
      "Epoch [34/50], Loss: 13.7891\n",
      "Epoch [35/50], Loss: 16.9145\n",
      "Epoch [35/50], Loss: 13.2042\n",
      "Epoch [35/50], Loss: 14.9375\n",
      "Epoch [35/50], Loss: 16.7342\n",
      "Epoch [35/50], Loss: 20.4253\n",
      "Epoch [35/50], Loss: 16.6842\n",
      "Epoch [35/50], Loss: 15.4044\n",
      "Epoch [35/50], Loss: 21.6624\n",
      "Epoch [35/50], Loss: 17.7259\n",
      "Epoch [35/50], Loss: 15.1843\n",
      "Epoch [35/50], Loss: 13.7885\n",
      "Epoch [36/50], Loss: 16.9146\n",
      "Epoch [36/50], Loss: 13.2042\n",
      "Epoch [36/50], Loss: 14.9376\n",
      "Epoch [36/50], Loss: 16.7342\n",
      "Epoch [36/50], Loss: 20.4254\n",
      "Epoch [36/50], Loss: 16.6842\n",
      "Epoch [36/50], Loss: 15.4042\n",
      "Epoch [36/50], Loss: 21.6627\n",
      "Epoch [36/50], Loss: 17.7262\n",
      "Epoch [36/50], Loss: 15.1843\n",
      "Epoch [36/50], Loss: 13.7883\n",
      "Epoch [37/50], Loss: 16.9146\n",
      "Epoch [37/50], Loss: 13.2043\n",
      "Epoch [37/50], Loss: 14.9376\n",
      "Epoch [37/50], Loss: 16.7341\n",
      "Epoch [37/50], Loss: 20.4255\n",
      "Epoch [37/50], Loss: 16.6842\n",
      "Epoch [37/50], Loss: 15.4040\n",
      "Epoch [37/50], Loss: 21.6630\n",
      "Epoch [37/50], Loss: 17.7265\n",
      "Epoch [37/50], Loss: 15.1843\n",
      "Epoch [37/50], Loss: 13.7882\n",
      "Epoch [38/50], Loss: 16.9146\n",
      "Epoch [38/50], Loss: 13.2043\n",
      "Epoch [38/50], Loss: 14.9377\n",
      "Epoch [38/50], Loss: 16.7341\n",
      "Epoch [38/50], Loss: 20.4256\n",
      "Epoch [38/50], Loss: 16.6842\n",
      "Epoch [38/50], Loss: 15.4038\n",
      "Epoch [38/50], Loss: 21.6633\n",
      "Epoch [38/50], Loss: 17.7268\n",
      "Epoch [38/50], Loss: 15.1843\n",
      "Epoch [38/50], Loss: 13.7883\n",
      "Epoch [39/50], Loss: 16.9146\n",
      "Epoch [39/50], Loss: 13.2044\n",
      "Epoch [39/50], Loss: 14.9377\n",
      "Epoch [39/50], Loss: 16.7340\n",
      "Epoch [39/50], Loss: 20.4258\n",
      "Epoch [39/50], Loss: 16.6842\n",
      "Epoch [39/50], Loss: 15.4035\n",
      "Epoch [39/50], Loss: 21.6638\n",
      "Epoch [39/50], Loss: 17.7273\n",
      "Epoch [39/50], Loss: 15.1843\n",
      "Epoch [39/50], Loss: 13.7884\n",
      "Epoch [40/50], Loss: 16.9146\n",
      "Epoch [40/50], Loss: 13.2045\n",
      "Epoch [40/50], Loss: 14.9379\n",
      "Epoch [40/50], Loss: 16.7338\n",
      "Epoch [40/50], Loss: 20.4263\n",
      "Epoch [40/50], Loss: 16.6842\n",
      "Epoch [40/50], Loss: 15.4028\n",
      "Epoch [40/50], Loss: 21.6652\n",
      "Epoch [40/50], Loss: 17.7287\n",
      "Epoch [40/50], Loss: 15.1843\n",
      "Epoch [40/50], Loss: 13.7889\n",
      "Epoch [41/50], Loss: 16.9147\n",
      "Epoch [41/50], Loss: 13.2047\n",
      "Epoch [41/50], Loss: 14.9382\n",
      "Epoch [41/50], Loss: 16.7334\n",
      "Epoch [41/50], Loss: 20.4273\n",
      "Epoch [41/50], Loss: 16.6842\n",
      "Epoch [41/50], Loss: 15.4011\n",
      "Epoch [41/50], Loss: 21.6683\n",
      "Epoch [41/50], Loss: 17.7317\n",
      "Epoch [41/50], Loss: 15.1843\n",
      "Epoch [41/50], Loss: 13.7900\n",
      "Epoch [42/50], Loss: 16.9147\n",
      "Epoch [42/50], Loss: 13.2051\n",
      "Epoch [42/50], Loss: 14.9390\n",
      "Epoch [42/50], Loss: 16.7326\n",
      "Epoch [42/50], Loss: 20.4297\n",
      "Epoch [42/50], Loss: 16.6842\n",
      "Epoch [42/50], Loss: 15.3969\n",
      "Epoch [42/50], Loss: 21.6757\n",
      "Epoch [42/50], Loss: 17.7391\n",
      "Epoch [42/50], Loss: 15.1844\n",
      "Epoch [42/50], Loss: 13.7924\n",
      "Epoch [43/50], Loss: 16.9145\n",
      "Epoch [43/50], Loss: 13.2052\n",
      "Epoch [43/50], Loss: 14.9394\n",
      "Epoch [43/50], Loss: 16.7319\n",
      "Epoch [43/50], Loss: 20.4320\n",
      "Epoch [43/50], Loss: 16.6843\n",
      "Epoch [43/50], Loss: 15.3927\n",
      "Epoch [43/50], Loss: 21.6838\n",
      "Epoch [43/50], Loss: 17.7477\n",
      "Epoch [43/50], Loss: 15.1844\n",
      "Epoch [43/50], Loss: 13.7959\n",
      "Epoch [44/50], Loss: 16.9142\n",
      "Epoch [44/50], Loss: 13.2044\n",
      "Epoch [44/50], Loss: 14.9387\n",
      "Epoch [44/50], Loss: 16.7325\n",
      "Epoch [44/50], Loss: 20.4310\n",
      "Epoch [44/50], Loss: 16.6844\n",
      "Epoch [44/50], Loss: 15.3930\n",
      "Epoch [44/50], Loss: 21.6839\n",
      "Epoch [44/50], Loss: 17.7492\n",
      "Epoch [44/50], Loss: 15.1845\n",
      "Epoch [44/50], Loss: 13.7975\n",
      "Epoch [45/50], Loss: 16.9139\n",
      "Epoch [45/50], Loss: 13.2030\n",
      "Epoch [45/50], Loss: 14.9367\n",
      "Epoch [45/50], Loss: 16.7346\n",
      "Epoch [45/50], Loss: 20.4254\n",
      "Epoch [45/50], Loss: 16.6842\n",
      "Epoch [45/50], Loss: 15.4035\n",
      "Epoch [45/50], Loss: 21.6676\n",
      "Epoch [45/50], Loss: 17.7310\n",
      "Epoch [45/50], Loss: 15.1842\n",
      "Epoch [45/50], Loss: 13.8018\n",
      "Epoch [46/50], Loss: 16.9138\n",
      "Epoch [46/50], Loss: 13.2031\n",
      "Epoch [46/50], Loss: 14.9369\n",
      "Epoch [46/50], Loss: 16.7346\n",
      "Epoch [46/50], Loss: 20.4254\n",
      "Epoch [46/50], Loss: 16.6842\n",
      "Epoch [46/50], Loss: 15.4046\n",
      "Epoch [46/50], Loss: 21.6655\n",
      "Epoch [46/50], Loss: 17.7275\n",
      "Epoch [46/50], Loss: 15.1841\n",
      "Epoch [46/50], Loss: 13.8040\n",
      "Epoch [47/50], Loss: 16.9137\n",
      "Epoch [47/50], Loss: 13.2030\n",
      "Epoch [47/50], Loss: 14.9371\n",
      "Epoch [47/50], Loss: 16.7343\n",
      "Epoch [47/50], Loss: 20.4263\n",
      "Epoch [47/50], Loss: 16.6843\n",
      "Epoch [47/50], Loss: 15.4024\n",
      "Epoch [47/50], Loss: 21.6692\n",
      "Epoch [47/50], Loss: 17.7319\n",
      "Epoch [47/50], Loss: 15.1841\n",
      "Epoch [47/50], Loss: 13.8029\n",
      "Epoch [48/50], Loss: 16.9138\n",
      "Epoch [48/50], Loss: 13.2032\n",
      "Epoch [48/50], Loss: 14.9372\n",
      "Epoch [48/50], Loss: 16.7341\n",
      "Epoch [48/50], Loss: 20.4269\n",
      "Epoch [48/50], Loss: 16.6842\n",
      "Epoch [48/50], Loss: 15.4010\n",
      "Epoch [48/50], Loss: 21.6713\n",
      "Epoch [48/50], Loss: 17.7348\n",
      "Epoch [48/50], Loss: 15.1842\n",
      "Epoch [48/50], Loss: 13.8014\n",
      "Epoch [49/50], Loss: 16.9138\n",
      "Epoch [49/50], Loss: 13.2030\n",
      "Epoch [49/50], Loss: 14.9368\n",
      "Epoch [49/50], Loss: 16.7345\n",
      "Epoch [49/50], Loss: 20.4258\n",
      "Epoch [49/50], Loss: 16.6842\n",
      "Epoch [49/50], Loss: 15.4029\n",
      "Epoch [49/50], Loss: 21.6681\n",
      "Epoch [49/50], Loss: 17.7313\n",
      "Epoch [49/50], Loss: 15.1842\n",
      "Epoch [49/50], Loss: 13.8009\n",
      "Epoch [50/50], Loss: 16.9138\n",
      "Epoch [50/50], Loss: 13.2029\n",
      "Epoch [50/50], Loss: 14.9366\n",
      "Epoch [50/50], Loss: 16.7348\n",
      "Epoch [50/50], Loss: 20.4247\n",
      "Epoch [50/50], Loss: 16.6843\n",
      "Epoch [50/50], Loss: 15.4050\n",
      "Epoch [50/50], Loss: 21.6642\n",
      "Epoch [50/50], Loss: 17.7271\n",
      "Epoch [50/50], Loss: 15.1841\n",
      "Epoch [50/50], Loss: 13.8001\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Unpack the batch\n",
    "        modalities, labels = batch\n",
    "        modality_1 = modalities[0]  # Each has shape (batch_size, feature, 1)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = [modality_1]\n",
    "        output = model(inputs)\n",
    "\n",
    "        #print(output, labels)\n",
    "        # Reshape labels to match the output shape if necessary\n",
    "        labels = labels.view(output.shape)  # Ensures labels has shape (batch_size, 1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, labels)\n",
    "        RMSE_loss = torch.sqrt(loss)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        RMSE_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {RMSE_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = loaders.MultiModalityDataset(features_csv[features_csv[\"split\"]==\"dev\"], \n",
    "                                            modalities = {\"audio\"}, label = \"PTSD_severity\")\n",
    "dev_dataloader = DataLoader(dev_loader, batch_size=16, collate_fn=loaders.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154],\n",
      "        [34.1154]])\n"
     ]
    }
   ],
   "source": [
    "# Model prediction\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch in dev_dataloader:\n",
    "        modalities, _ = batch  # Ignore labels if unavailable\n",
    "        modality_1 = modalities[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = [modality_1]\n",
    "        output = model(inputs)  # Model's prediction\n",
    "        \n",
    "        # Collect predictions\n",
    "        predictions.append(output)\n",
    "\n",
    "# Combine predictions into a single tensor\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "y_pred = np.array(predictions)\n",
    "print(f\"Predictions:\\n{predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(features_csv[features_csv[\"split\"]==\"dev\"][\"PTSD_severity\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_csv['col_encoded'] = features_csv['gender'].map({'female': 0, 'male': 1, None:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1., nan])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(features_csv['col_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of gender_test: (56,)\n",
      "Shape of y_test: (56,)\n",
      "Shape of y_pred: (56, 1)\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference\n",
    "\n",
    "# Assuming `gender` corresponds to the original metadata\n",
    "# Split the dataset into training and testing sets for `gender`\n",
    "gender_train, gender_test = features_csv[features_csv[\"split\"]==\"train\"][\"col_encoded\"], features_csv[features_csv[\"split\"]==\"dev\"][\"col_encoded\"]\n",
    "\n",
    "# Ensure that `gender_test`, `y_test`, and `y_pred` are aligned\n",
    "print(f\"Shape of gender_test: {gender_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "print(f\"Shape of y_pred: {y_pred.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4      1.0\n",
       "5      NaN\n",
       "6      0.0\n",
       "11     1.0\n",
       "14     1.0\n",
       "15     1.0\n",
       "24     1.0\n",
       "29     1.0\n",
       "30     0.0\n",
       "32     1.0\n",
       "36     1.0\n",
       "43     0.0\n",
       "46     1.0\n",
       "54     1.0\n",
       "61     0.0\n",
       "66     1.0\n",
       "72     0.0\n",
       "74     0.0\n",
       "76     1.0\n",
       "77     0.0\n",
       "80     0.0\n",
       "81     1.0\n",
       "89     1.0\n",
       "94     1.0\n",
       "103    0.0\n",
       "111    0.0\n",
       "119    0.0\n",
       "124    1.0\n",
       "126    1.0\n",
       "138    1.0\n",
       "143    0.0\n",
       "147    0.0\n",
       "149    0.0\n",
       "150    1.0\n",
       "158    1.0\n",
       "163    1.0\n",
       "164    0.0\n",
       "167    1.0\n",
       "172    1.0\n",
       "184    0.0\n",
       "189    1.0\n",
       "190    0.0\n",
       "193    1.0\n",
       "197    1.0\n",
       "206    0.0\n",
       "208    1.0\n",
       "212    1.0\n",
       "223    1.0\n",
       "234    1.0\n",
       "238    1.0\n",
       "239    1.0\n",
       "247    0.0\n",
       "249    0.0\n",
       "253    1.0\n",
       "268    1.0\n",
       "269    1.0\n",
       "Name: col_encoded, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Parity Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate Demographic Parity Difference\n",
    "demographic_parity = demographic_parity_difference(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred[:,0],\n",
    "    sensitive_features=gender_test\n",
    ")\n",
    "\n",
    "print(f\"Demographic Parity Difference: {demographic_parity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection Rates by Group:\n",
      "gender\n",
      "female     0.0\n",
      "male       0.0\n",
      "unknown    0.0\n",
      "Name: selection_rate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "metric_frame = MetricFrame(metrics=selection_rate, \n",
    "                           y_true=y_test, \n",
    "                           y_pred=y_pred[:,0], \n",
    "                           sensitive_features=gender_test)\n",
    "\n",
    "# Get selection rates for each group\n",
    "selection_rates = metric_frame.by_group\n",
    "print(\"Selection Rates by Group:\")\n",
    "print(selection_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/954025/ipykernel_2837344/1510568790.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  group_a_rate = selection_rates[1]\n",
      "/scratch/954025/ipykernel_2837344/1510568790.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  group_b_rate = selection_rates[0]\n"
     ]
    }
   ],
   "source": [
    "group_a_rate = selection_rates[1]\n",
    "group_b_rate = selection_rates[0]\n",
    "\n",
    "if group_b_rate > 0:  # Avoid division by zero\n",
    "    demographic_parity_ratio = group_a_rate / group_b_rate\n",
    "    print(f\"Demographic Parity Ratio: {demographic_parity_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
